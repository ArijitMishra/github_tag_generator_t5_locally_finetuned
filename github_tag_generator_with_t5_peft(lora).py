# -*- coding: utf-8 -*-
"""GitHub Tag Generator with T5 + PEFT(LoRA).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qJhROfQmmb4_ShOLtStH4_UboJt87XM0
"""
import os

from dotenv import load_dotenv


load_dotenv()
#os.environ["HF_TOKEN"] = os.environ("HF_TOKEN")

from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import AutoTokenizer, Trainer, DataCollatorForSeq2Seq
from datasets import load_dataset, DatasetDict
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftConfig
import wandb

os.environ["WANDB_PROJECT"]="T5 finetune locally for github tags generation"



tokenizer = T5Tokenizer.from_pretrained("t5-small")

dataset = load_dataset("zamal/github-meta-data")

split = dataset["train"].train_test_split(test_size=0.1, seed=42)
dataset_dict = DatasetDict({"train": split["train"], "validation": split["test"]})

print(len(dataset_dict["train"]))
print(len(dataset_dict["validation"]))

model_name = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess(batch):
  inputs = batch["input"]
  targets = batch["target"]
  model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
  labels = tokenizer(targets, max_length=64, truncation=True, padding="max_length").input_ids
  model_inputs["labels"] = labels
  return model_inputs

tokenized = dataset_dict.map(preprocess, batched=True, remove_columns=dataset_dict["train"].column_names)
tokenized.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

model = T5ForConditionalGeneration.from_pretrained(model_name)

lora_config = LoraConfig(
    r = 16,
    lora_alpha=16,
    target_modules=["q", "v"],
    lora_dropout = 0.5,
    bias = "none",
    task_type = "SEQ_2_SEQ_LM"
)

model = get_peft_model(model, lora_config)

training_args = TrainingArguments(
    output_dir="./t5_tag_generator",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=1e-4,
    num_train_epochs=100,
    logging_steps=10,
    eval_strategy="steps",
    eval_steps=50,
    save_steps=50,
    save_total_limit=2,
    fp16=True,
    push_to_hub=True,
    hub_model_id="ArijitMishra/t5_model_finetuned_github_tag_generator_from_local",
    hub_token=os.environ["HF_TOKEN"],
    report_to="wandb",
    # load_best_model_at_end=True
)

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer
)

trainer.train()

